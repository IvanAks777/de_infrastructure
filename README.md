# 🚀 Data Engineering Infrastructure Project

![Data Engineering](https://img.shields.io/badge/Data-Engineering-blue)
![Docker](https://img.shields.io/badge/Docker-Compose-orange)
![Apache](https://img.shields.io/badge/Apache-Ecosystem-red)


## 🌐 Overview

This project provides a complete **data engineering infrastructure** setup using modern tools and technologies. It includes workflow orchestration, data processing, storage, and visualization components.

## 🔑 Users and Access

### 🔒 Database Credentials
- **Airflow DB**: `airflow/airflow`
- **App PostgreSQL**: `appuser/apppassword`
- **Superset PostgreSQL**: `superset/superset_password`
- **ClickHouse**: `clickhouse/clickhouse`

### ⚙️ Service Credentials
- **MinIO**:
  - Root: `minio/minio123`
  - Access: `minio/minio`
- **Airflow Web UI**: `${_AIRFLOW_WWW_USER_USERNAME}/${_AIRFLOW_WWW_USER_PASSWORD}` (from .env)
- **Superset Admin**: `admin/admin`
- **Jupyter**: root user (no password)

> **Note**
> Credentials marked with `${VARIABLE}` should be set in the `.env` file before starting services.

## 🧩 Main Components

### ⏱️ Workflow Orchestration
- **Apache Airflow**: For scheduling and monitoring workflows
  - Located in `airflow/` directory
  - Includes sample DAGs in `airflow/dags/`

### 🔄 Data Processing
- **Apache Spark**: For batch and stream processing
  - Spark configuration in `spark/conf/`
  - Sample applications in `spark/app/`

### 💾 Storage
- **MinIO**: S3-compatible object storage
  - Sample data stored in `minio/` directory
- **PostgreSQL**: Relational database
  - Configuration in `postgres/` directory

### 📊 Analytics & Visualization
- **Jupyter Notebooks**: For data exploration
  - Located in `notebooks/` directory
- **Apache Superset**: For data visualization
  - Configuration in `config/superset_config.py`

## 🛠️ Setup Instructions

1. Ensure Docker and Docker Compose are installed
2. Clone this repository
3. Run `docker-compose up -d` to start all services
4. Access services:
   - **Airflow**: http://localhost:8080 (workflow orchestration)
   - **Jupyter**: http://localhost:8889 (notebooks)
   - **Superset**: http://localhost:8088 (visualization)
   - **MinIO**:
     - API: http://localhost:9000
     - Console: http://localhost:9001
   - **Kafka**:
     - Broker: localhost:9092 (internal)
     - External: localhost:29092
   - **Kafka UI**: http://localhost:8082 (message queue monitoring)
   - **Spark UI**: http://localhost:4040 (job monitoring)
   - **ClickHouse**: http://localhost:8123 (analytical DB)

## 📂 Directory Structure

```
.
├── airflow/            # Airflow configuration and DAGs
├── config/            # Configuration files
├── data/              # Sample data files
├── minio/             # MinIO object storage
├── notebooks/         # Jupyter notebooks
├── postgres/         # PostgreSQL configuration
├── scripts/          # Utility scripts
├── spark/            # Spark configuration and apps
├── docker-compose.yml # Main compose file
└── README.md         # This file
```

## 💡 Usage Examples

### 🚀 Running a Spark Job
```bash
docker-compose exec spark spark-submit /opt/spark/app/your_app.py
```

### ⏱️ Creating a new Airflow DAG
1. Add your Python DAG file to `airflow/dags/`
2. Airflow will automatically pick it up

### 📦 Accessing MinIO
- Access key: `minioadmin`
- Secret key: `minioadmin`
- Bucket: `new` (contains sample data)
