# ğŸš€ Data Engineering Infrastructure Project

![Data Engineering](https://img.shields.io/badge/Data-Engineering-blue)
![Docker](https://img.shields.io/badge/Docker-Compose-orange)
![Apache](https://img.shields.io/badge/Apache-Ecosystem-red)


## ğŸŒ Overview

This project provides a complete **data engineering infrastructure** setup using modern tools and technologies. It includes workflow orchestration, data processing, storage, and visualization components.

## ğŸ”‘ Users and Access

### ğŸ”’ Database Credentials
- **Airflow DB**: `airflow/airflow`
- **App PostgreSQL**: `appuser/apppassword`
- **Superset PostgreSQL**: `superset/superset_password`
- **ClickHouse**: `clickhouse/clickhouse`

### âš™ï¸ Service Credentials
- **MinIO**:
  - Root: `minio/minio123`
  - Access: `minio/minio`
- **Airflow Web UI**: `${_AIRFLOW_WWW_USER_USERNAME}/${_AIRFLOW_WWW_USER_PASSWORD}` (from .env)
- **Superset Admin**: `admin/admin`
- **Jupyter**: root user (no password)

> **Note**
> Credentials marked with `${VARIABLE}` should be set in the `.env` file before starting services.

## ğŸ§© Main Components

### â±ï¸ Workflow Orchestration
- **Apache Airflow**: For scheduling and monitoring workflows
  - Located in `airflow/` directory
  - Includes sample DAGs in `airflow/dags/`

### ğŸ”„ Data Processing
- **Apache Spark**: For batch and stream processing
  - Spark configuration in `spark/conf/`
  - Sample applications in `spark/app/`

### ğŸ’¾ Storage
- **MinIO**: S3-compatible object storage
  - Sample data stored in `minio/` directory
- **PostgreSQL**: Relational database
  - Configuration in `postgres/` directory

### ğŸ“Š Analytics & Visualization
- **Jupyter Notebooks**: For data exploration
  - Located in `notebooks/` directory
- **Apache Superset**: For data visualization
  - Configuration in `config/superset_config.py`

## ğŸ› ï¸ Setup Instructions

1. Ensure Docker and Docker Compose are installed
2. Clone this repository
3. Run `docker-compose up -d` to start all services
4. Access services:
   - **Airflow**: http://localhost:8080 (workflow orchestration)
   - **Jupyter**: http://localhost:8889 (notebooks)
   - **Superset**: http://localhost:8088 (visualization)
   - **MinIO**:
     - API: http://localhost:9000
     - Console: http://localhost:9001
   - **Kafka**:
     - Broker: localhost:9092 (internal)
     - External: localhost:29092
   - **Kafka UI**: http://localhost:8082 (message queue monitoring)
   - **Spark UI**: http://localhost:4040 (job monitoring)
   - **ClickHouse**: http://localhost:8123 (analytical DB)

## ğŸ“‚ Directory Structure

```
.
â”œâ”€â”€ airflow/            # Airflow configuration and DAGs
â”œâ”€â”€ config/            # Configuration files
â”œâ”€â”€ data/              # Sample data files
â”œâ”€â”€ minio/             # MinIO object storage
â”œâ”€â”€ notebooks/         # Jupyter notebooks
â”œâ”€â”€ postgres/         # PostgreSQL configuration
â”œâ”€â”€ scripts/          # Utility scripts
â”œâ”€â”€ spark/            # Spark configuration and apps
â”œâ”€â”€ docker-compose.yml # Main compose file
â””â”€â”€ README.md         # This file
```

## ğŸ’¡ Usage Examples

### ğŸš€ Running a Spark Job
```bash
docker-compose exec spark spark-submit /opt/spark/app/your_app.py
```

### â±ï¸ Creating a new Airflow DAG
1. Add your Python DAG file to `airflow/dags/`
2. Airflow will automatically pick it up

### ğŸ“¦ Accessing MinIO
- Access key: `minioadmin`
- Secret key: `minioadmin`
- Bucket: `new` (contains sample data)
